{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 459.18351459503174\n",
      "(1306122, 70) (56370, 70) (1306122,) 94999\n",
      "time : 67.73985266685486\n",
      ">>> 1: \n",
      "Epoch [1/4] - time: 284.54s\n",
      "Epoch [2/4] - time: 284.70s\n",
      "Epoch [3/4] - time: 285.39s\n",
      "Epoch [4/4] - time: 285.49s\n",
      ">>> 2: \n",
      "Epoch [1/4] - time: 284.87s\n",
      "Epoch [2/4] - time: 285.41s\n",
      "Epoch [3/4] - time: 285.51s\n",
      "Epoch [4/4] - time: 285.46s\n",
      ">>> 3: \n",
      "Epoch [1/4] - time: 390.00s\n",
      "Epoch [2/4] - time: 390.06s\n",
      "Epoch [3/4] - time: 390.14s\n",
      "Epoch [4/4] - time: 390.05s\n",
      ">>> 4: \n",
      "Epoch [1/4] - time: 390.15s\n",
      "Epoch [2/4] - time: 390.07s\n",
      "Epoch [3/4] - time: 390.06s\n",
      "Epoch [4/4] - time: 390.07s\n",
      ">>>Total runtime 1:38:56.990126.\n",
      " 阀值: 0.3807620794631004\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from __future__ import division, print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from sklearn import model_selection, preprocessing, metrics, linear_model\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import re\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim\n",
    "import nltk\n",
    "from wordcloud import STOPWORDS\n",
    "import operator\n",
    "from collections import Counter\n",
    "\n",
    "begin_time = datetime.now()\n",
    "\n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 70 # max number of words in a question to use\n",
    "batch_size = 512\n",
    "seed = 1029\n",
    "\n",
    "val = False\n",
    "gpu_device_count = '0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_device_count   #指定第一块GPU可用\n",
    "\n",
    "def set_seed(seed=2018):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    tf.set_random_seed(seed)\n",
    "set_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "puncts=['ɖ', '✔', ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^', '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', '谢', '六', '佬', '|', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^', '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', 'é', '&amp;', '₹', 'á', '²', 'ế', '청', '하', '¨', '‘', '√', '\\xa0', '高', '端', '大', '气', '上', '档', '次', '_', '½', 'π', '#', '小', '鹿', '乱', '撞', '成', '语', 'ë', 'à', 'ç', '@', 'ü', 'č', 'ć', 'ž', 'đ', '°', 'द', 'े', 'श', '्', 'र', 'ो', 'ह', 'ि', 'प', 'स', 'थ', 'त', 'न', 'व', 'ा', 'ल', 'ं', '林', '彪', '€', '\\u200b', '˚', 'ö', '~', '—', '越', '人', 'च', 'म', 'क', 'ु', 'य', 'ी', 'ê', 'ă', 'ễ', '∞', '抗', '日', '神', '剧', '，', '\\uf02d', '–', 'ご', 'め', 'な', 'さ', 'い', 'す', 'み', 'ま', 'せ', 'ん', 'ó', 'è', '£', '¡', 'ś', '≤', '¿', 'λ', '魔', '法', '师', '）', 'ğ', 'ñ', 'ř', '그', '자', '식', '멀', '쩡', '다', '인', '공', '호', '흡', '데', '혀', '밀', '어', '넣', '는', '거', '보', '니', 'ǒ', 'ú', '️', 'ش', 'ه', 'ا', 'د', 'ة', 'ل', 'ت', 'َ', 'ع', 'م', 'ّ', 'ق', 'ِ', 'ف', 'ي', 'ب', 'ح', 'ْ', 'ث', '³', '饭', '可', '以', '吃', '话', '不', '讲', '∈', 'ℝ', '爾', '汝', '文', '言', '∀', '禮', 'इ', 'ब', 'छ', 'ड', '़', 'ʒ', '有', '「', '寧', '錯', '殺', '一', '千', '絕', '放', '過', '」', '之', '勢', '㏒', '㏑', 'ू', 'â', 'ω', 'ą', 'ō', '精', '杯', 'í', '生', '懸', '命', 'ਨ', 'ਾ', 'ਮ', 'ੁ', '₁', '₂', 'ϵ', 'ä', 'к', 'ɾ', '\\ufeff', 'ã', '©', '\\x9d', 'ū', '™', '＝', 'ù', 'ɪ', 'ŋ', 'خ', 'ر', 'س', 'ن', 'ḵ', 'ā']\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def token_text(x):\n",
    "    x = str(x)\n",
    "    return nltk.word_tokenize(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "\n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim),\n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "\n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "\n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "\n",
    "\n",
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma ** (x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_and_prec():\n",
    "    train_df = pd.read_csv(\"../input/train.csv\")\n",
    "    #     test_df = pd.read_csv(\"../input/test.csv\")\n",
    "    if val:\n",
    "        test_df = train_df[:130000].reset_index(drop=True)\n",
    "        train_df = train_df[130000:].reset_index(drop=True)\n",
    "    else:\n",
    "        test_df = pd.read_csv(\"../input/test.csv\")\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].str.lower()\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].str.lower()\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "    train_df[\"question_text\"].fillna(\"_##_\")\n",
    "    test_df[\"question_text\"].fillna(\"_##_\")\n",
    "\n",
    "    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: token_text(x))\n",
    "    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: token_text(x))\n",
    "    train_X = train_df[\"question_text\"].values\n",
    "    test_X = test_df[\"question_text\"].values\n",
    "\n",
    "    pos_df = train_df[(train_df['target'] == 1)]\n",
    "    pos_X = train_df['question_text'].values\n",
    "    counts = Counter()\n",
    "\n",
    "    for sentence in pos_X:\n",
    "        counts.update(sentence)\n",
    "        counts.update(sentence)\n",
    "\n",
    "    for sentence in train_X:\n",
    "        sentence = list(set(sentence))\n",
    "        counts.update(sentence)\n",
    "    counts = sorted(counts.items(), key=operator.itemgetter(1))\n",
    "    counts.reverse()\n",
    "    counts = dict(counts)\n",
    "    \n",
    "    \n",
    "    ##mispell words\n",
    "    mis_words_dic={'britebart': 'breitbart', 'earg': 'urge', 'houthies': 'houthis', 'bipan': 'bipin', 'hags': 'hag', 'bullyboy': 'bully', 'fyck': 'fuck', 'dwelve': 'delve', 'radivals': 'radical', 'winy': 'whiny', 'mysoginists': 'misogynist', 'catergorized': 'categorized', 'consecrated': 'consecrate', 'agancy': 'agency', 'cuckservatives': 'conservatives', 'gerrymanders': 'gerrymander', 'canadia': 'canadian', 'babaric': 'barack', 'schoolteachers': 'teachers', 'kebba': 'kaaba', 'ladys': 'ladies', 'gunarathne': 'gunaratne', 'howmother': 'how', 'cooties': 'cock', 'teeenage': 'teenage', 'owaesi': 'owaisi', 'supplants': 'supplant', 'terming': 'term', 'mongolianz': 'mongolians', 'extorts': 'extort', 'tertullians': 'tertullian', 'paraniod': 'paranoid', 'wakandan': 'wakanda', 'transgeneric': 'transgender', 'himen': 'hymen', 'illbred': 'bred', 'transgeder': 'transgender', 'goofle': 'google', 'persecutor': 'prosecutor', 'fucktard': 'fuck', 'emboldens': 'embolden', 'jalikatu': 'jallikattu', 'fashist': 'fascist', 'earf': 'earth', 'causians': 'caucasians', 'dishornable': 'dishonorable', 'antimodi': 'modi', 'detoxifying': 'detoxing', 'preodential': 'presidential', 'ccausing': 'causing', 'lyncing': 'lynching', 'exonorated': 'exonerated', 'woemn': 'women', 'cinhese': 'chinese', 'holoucast': 'holocaust', 'kenpeitai': 'kempeitai', 'fonies': 'phonies', 'bibicial': 'biblical', 'antiscience': 'anti', 'grothesque': 'grotesque', 'stokholm': 'stockholm', 'homophilia': 'homosexuality', 'gorrilas': 'gorillas', 'chaibala': 'chaiwala', 'teluguians': 'telugu', 'hadrat': 'hazrat', 'ᗰoᖇe': 'more', 'nonpracticing': 'practicing', 'statistucs': 'statistics', 'unacceptabing': 'unacceptable', 'globed': 'globe', 'sommons': 'summons', 'ussually': 'usually', 'centimiters': 'centimeters', 'phoniness': 'phony', 'overral': 'overall', \"apist\": \"rapist\", \"pachabottu\": \"pacha\", \"mycaster\": \"code\", \"loundly\": \"loudly\", \"fetxh\": \"data\", \"clickbait\": \"click\", \"tigernut\": \"tiger\", \"indiaqr\": \"code\", \"narcist\": \"narcissist\", \"quorreled\": \"quarreled\", \"bharathnatyam\": \"bharatnatyam\", \"sinnister\": \"sinister\", \"careamics\": \"ceramics\", \"diffferently\": \"differently\", \"kubernetes\": \"software\", \"maintanable\": \"maintainable\", \"nikodym\": \"radon\", \"thunderstike\": \"thunderstrike\", \"thaads\": \"thaad\", \"arichtecture\": \"architecture\", \"simlarity\": \"similarity\", \"walmartlabs\": \"walmart\", \"padmaavat\": \"padmavati\", \"swagatham\":\"program\",\"cleanshot\": \"clean\", \"bandhup\": \"bhandup\", \"ramk\": \"rank\", \"examinaton\": \"examination\", \"imucet\": \"cet\", \"mongorestore\": \"linux\", \"mongodump\": \"linux\", \"pakkstani\": \"pakistani\", \"venuas\": \"venus\", \"savegely\": \"savagely\", \"redmi\": \"iphone\", \"anizara\": \"aniraza\", \"nanodegree\": \"degree\", \"calead\": \"leap\"}\n",
    "    mis_words = set(list(mis_words_dic.keys()))\n",
    "\n",
    "    oov_words = []\n",
    "    all_words_dic = {}\n",
    "    word_dict = {}\n",
    "    j = 1\n",
    "\n",
    "    for word, i in counts.items():\n",
    "        if word in mis_words:\n",
    "            oov_words.append(word)\n",
    "            continue\n",
    "        word_dict[word] = j\n",
    "        all_words_dic[word] = j\n",
    "        j += 1\n",
    "        if j == max_features:\n",
    "            break\n",
    "    for word in oov_words:\n",
    "        try:\n",
    "            all_words_dic[word] = word_dict[mis_words_dic[word]]\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    def pad_sentence(sentence):\n",
    "        ####把问号放在最后面\n",
    "        lenth = len(sentence)\n",
    "        list1 = []\n",
    "        temp = []\n",
    "        for i in range(lenth):\n",
    "            word = sentence[i]\n",
    "            temp.append(word)\n",
    "            if word == '?':\n",
    "                list1 = temp + list1\n",
    "                temp = []\n",
    "        list1 = temp + list1\n",
    "        ###把问号放在最后面\n",
    "        list1.reverse()\n",
    "        list2 = []\n",
    "        list3 = []\n",
    "        for word in list1:\n",
    "            try:\n",
    "                list3.append(all_words_dic[word])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        l1 = len(list3)\n",
    "        for i in range(maxlen):\n",
    "            if i < l1:\n",
    "                list2.append(list3[i])\n",
    "            else:\n",
    "                list2.append(0)\n",
    "        list2.reverse()\n",
    "        return list2\n",
    "\n",
    "    if val:\n",
    "        train_x = []\n",
    "        for sentence in train_X:\n",
    "            train_x.append(pad_sentence(sentence))\n",
    "        test_x = []\n",
    "        for sentence in test_X:\n",
    "            test_x.append(pad_sentence(sentence))\n",
    "        train_y = train_df['target'].values\n",
    "        test_y = test_df['target'].values\n",
    "        train_x = np.array(train_x)\n",
    "        test_x = np.array(test_x)\n",
    "        return train_x, test_x, train_y, test_y, word_dict\n",
    "    else:\n",
    "        train_x = []\n",
    "        for sentence in train_X:\n",
    "            train_x.append(pad_sentence(sentence))\n",
    "        test_x = []\n",
    "        for sentence in test_X:\n",
    "            test_x.append(pad_sentence(sentence))\n",
    "        train_y = train_df['target'].values\n",
    "        train_x = np.array(train_x)\n",
    "        test_x = np.array(test_x)\n",
    "        return train_x, test_x, train_y, word_dict\n",
    "\n",
    "\n",
    "# **Load embeddings**\n",
    "def load_glove_fast(word_index):\n",
    "    up_dict = {}\n",
    "    mask = np.zeros((max_features,))\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    with open(EMBEDDING_FILE, 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word not in word_index:\n",
    "                if word.lower() not in word_index:\n",
    "                    continue\n",
    "                embedding_vector = np.asarray(vec.split(' '), dtype='float32')[:300]\n",
    "                if len(embedding_vector) == 300:\n",
    "                    up_dict[word.lower()] = embedding_vector\n",
    "                continue\n",
    "            i = word_index[word]\n",
    "            embedding_vector = np.asarray(vec.split(' '), dtype='float32')[:300]\n",
    "            if len(embedding_vector) == 300:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                mask[i] = 1\n",
    "    return embedding_matrix, mask\n",
    "\n",
    "def load_para_fast(word_index):\n",
    "    up_dict = {}\n",
    "    mask = np.zeros((max_features,))\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    with open(EMBEDDING_FILE, 'r', encoding=\"utf8\", errors='ignore') as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word not in word_index:\n",
    "                if word.lower() not in word_index:\n",
    "                    continue\n",
    "                embedding_vector = np.asarray(vec.split(' '), dtype='float32')[:300]\n",
    "                if len(embedding_vector) == 300:\n",
    "                    up_dict[word.lower()] = embedding_vector\n",
    "                continue\n",
    "            i = word_index[word]\n",
    "            embedding_vector = np.asarray(vec.split(' '), dtype='float32')[:300]\n",
    "            if len(embedding_vector) == 300:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                mask[i] = 1\n",
    "    return embedding_matrix, mask\n",
    "\n",
    "def load_fast_fast(word_index):\n",
    "    up_dict = {}\n",
    "    mask = np.zeros((max_features,))\n",
    "    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    with open(EMBEDDING_FILE, 'r', encoding=\"utf8\", errors='ignore') as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word not in word_index:\n",
    "                if word.lower() not in word_index:\n",
    "                    continue\n",
    "                embedding_vector = np.asarray(vec.split(' '), dtype='float32')[:300]\n",
    "                if len(embedding_vector) == 300:\n",
    "                    up_dict[word.lower()] = embedding_vector\n",
    "                continue\n",
    "            i = word_index[word]\n",
    "            embedding_vector = np.asarray(vec.split(' '), dtype='float32')[:300]\n",
    "            if len(embedding_vector) == 300:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                mask[i] = 1\n",
    "    return embedding_matrix, mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''split-data'''\n",
    "submit_mode = 0\n",
    "if val:\n",
    "    path = './spnpy_val'\n",
    "else:\n",
    "    path = './spnpy_sub'\n",
    "if submit_mode == 0:\n",
    "    start_time = time.time()\n",
    "    if val:\n",
    "        train_X, test_X, train_y, test_y, word_index = load_and_prec()\n",
    "    else:\n",
    "        train_X, test_X, train_y, word_index = load_and_prec()\n",
    "    print(\"time :\", time.time() - start_time)\n",
    "    print(train_X.shape, test_X.shape, train_y.shape, len(word_index))\n",
    "\n",
    "    start_time = time.time()\n",
    "    embedding_matrix_g, mask_g = load_glove_fast(word_index)\n",
    "    embedding_matrix_p, mask_p = load_para_fast(word_index)\n",
    "    embedding_matrix_f, mask_f = load_fast_fast(word_index)\n",
    "    embedding_matrix_gp = np.mean([embedding_matrix_g, embedding_matrix_p, embedding_matrix_f], axis=0)\n",
    "    print(\"time :\", time.time() - start_time)\n",
    "\n",
    "elif submit_mode == 1:\n",
    "    start_time = time.time()\n",
    "    if val:\n",
    "        train_X, test_X, train_y, test_y, word_index = load_and_prec()\n",
    "    else:\n",
    "        train_X, test_X, train_y, word_index = load_and_prec()\n",
    "    print(\"time :\", time.time() - start_time)\n",
    "    print(train_X.shape, test_X.shape, train_y.shape, len(word_index))\n",
    "\n",
    "    start_time = time.time()\n",
    "    embedding_matrix_g, mask_g = load_glove_fast(word_index)\n",
    "    embedding_matrix_p, mask_p = load_para_fast(word_index)\n",
    "    embedding_matrix_f, mask_f = load_fast_fast(word_index)\n",
    "    embedding_matrix_gp = np.mean([embedding_matrix_g, embedding_matrix_p, embedding_matrix_f], axis=0)\n",
    "    print(\"time :\", time.time() - start_time)\n",
    "\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    np.save(path + \"/X_train.npy\", train_X)\n",
    "    np.save(path + \"/X_test.npy\", test_X)\n",
    "    np.save(path + \"/y_train.npy\", train_y)\n",
    "    if val:\n",
    "        np.save(path + \"/y_test.npy\", test_y)\n",
    "    np.save(path + \"/vocab.npy\", word_index)\n",
    "\n",
    "    np.save(path + \"/embedding_matrix_g.npy\", embedding_matrix_g)\n",
    "    np.save(path + \"/embedding_matrix_p.npy\", embedding_matrix_p)\n",
    "    np.save(path + \"/embedding_matrix_gp.npy\", embedding_matrix_gp)\n",
    "\n",
    "elif submit_mode == 2:\n",
    "    train_X = np.load(path + \"/X_train.npy\")\n",
    "    test_X = np.load(path + \"/X_test.npy\")\n",
    "    train_y = np.load(path + \"/y_train.npy\")\n",
    "    if val:\n",
    "        test_y = np.load(path + \"/y_test.npy\")\n",
    "    word_index = np.load(path + \"/vocab.npy\")\n",
    "    print(train_X.shape, test_X.shape, train_y.shape, test_y.shape)\n",
    "\n",
    "    embedding_matrix_g = np.load(path + \"/embedding_matrix_g.npy\")\n",
    "    embedding_matrix_p = np.load(path + \"/embedding_matrix_p.npy\")\n",
    "    embedding_matrix_gp = np.load(path + \"/embedding_matrix_gp.npy\")\n",
    "\n",
    "\n",
    "if val:\n",
    "    x_test_cuda = torch.tensor(test_X, dtype=torch.long).cuda()\n",
    "    y_test_cuda = torch.tensor(test_y[:, np.newaxis], dtype=torch.float32).cuda()\n",
    "    test = torch.utils.data.TensorDataset(x_test_cuda, y_test_cuda)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    x_train_fold = torch.tensor(train_X, dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(train_y[:, np.newaxis], dtype=torch.float32).cuda()\n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "else:\n",
    "    x_test_cuda = torch.tensor(test_X, dtype=torch.long).cuda()\n",
    "    test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    x_train_fold = torch.tensor(train_X, dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(train_y[:, np.newaxis], dtype=torch.float32).cuda()\n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# ****************************** model 1 ******************************\n",
    "def model1(epochs=4, embedding_matrix=None, clip=False):\n",
    "    hidden_size = 60\n",
    "    gru_len = hidden_size\n",
    "    Routings = 4\n",
    "    Num_capsule = 5\n",
    "    Dim_capsule = 5\n",
    "    T_epsilon = 1e-7\n",
    "\n",
    "    \n",
    "    class Caps_Layer(nn.Module):\n",
    "        def __init__(self, input_dim_capsule=gru_len * 2, num_capsule=Num_capsule, dim_capsule=Dim_capsule, \\\n",
    "                     routings=Routings, kernel_size=(9, 1), share_weights=True,\n",
    "                     activation='default', **kwargs):\n",
    "            super(Caps_Layer, self).__init__(**kwargs)\n",
    "\n",
    "            self.num_capsule = num_capsule\n",
    "            self.dim_capsule = dim_capsule\n",
    "            self.routings = routings\n",
    "            self.kernel_size = kernel_size  # 暂时没用到\n",
    "            self.share_weights = share_weights\n",
    "            if activation == 'default':\n",
    "                self.activation = self.squash\n",
    "            else:\n",
    "                self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "            if self.share_weights:\n",
    "                self.W = nn.Parameter(\n",
    "                    nn.init.xavier_normal_(torch.empty(1, input_dim_capsule, self.num_capsule * self.dim_capsule)))\n",
    "            else:\n",
    "                self.W = nn.Parameter(\n",
    "                    torch.randn(BATCH_SIZE, input_dim_capsule, self.num_capsule * self.dim_capsule))\n",
    "\n",
    "        def forward(self, x):\n",
    "            if self.share_weights:\n",
    "                u_hat_vecs = torch.matmul(x, self.W)\n",
    "            else:\n",
    "                print('add later')\n",
    "            batch_size = x.size(0)\n",
    "            input_num_capsule = x.size(1)\n",
    "            u_hat_vecs = u_hat_vecs.view((batch_size, input_num_capsule,\n",
    "                                          self.num_capsule, self.dim_capsule))\n",
    "            u_hat_vecs = u_hat_vecs.permute(0, 2, 1, 3)\n",
    "            b = torch.zeros_like(u_hat_vecs[:, :, :, 0])\n",
    "\n",
    "            for i in range(self.routings):\n",
    "                b = b.permute(0, 2, 1)\n",
    "                c = F.softmax(b, dim=2)\n",
    "                c = c.permute(0, 2, 1)\n",
    "                b = b.permute(0, 2, 1)\n",
    "                outputs = self.activation(torch.einsum('bij,bijk->bik', (c, u_hat_vecs)))\n",
    "                \n",
    "                if i < self.routings - 1:\n",
    "                    b = torch.einsum('bik,bijk->bij', (outputs, u_hat_vecs))\n",
    "            return outputs  # (batch_size, num_capsule, dim_capsule)\n",
    "\n",
    "        def squash(self, x, axis=-1):\n",
    "            s_squared_norm = (x ** 2).sum(axis, keepdim=True)\n",
    "            scale = torch.sqrt(s_squared_norm + T_epsilon)\n",
    "            return x / scale\n",
    "\n",
    "\n",
    "    class NeuralNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(NeuralNet, self).__init__()\n",
    "\n",
    "            linear_unit = 16\n",
    "            linear_unit1 = 16\n",
    "\n",
    "            self.embedding = nn.Embedding(max_features, embed_size)\n",
    "            self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "            self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "            self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "            self.gru = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "            self.lstm2 = nn.LSTM(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "            self.lstm_atten = Attention(hidden_size * 2, maxlen)\n",
    "            self.gru_atten = Attention(hidden_size * 2, maxlen)\n",
    "\n",
    "            self.linear = nn.Linear(hidden_size * 8 + 1, linear_unit1)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.fc = nn.Linear(linear_unit ** 2, linear_unit)\n",
    "            self.out = nn.Linear(linear_unit, 1)\n",
    "            self.lincaps = nn.Linear(Num_capsule * Dim_capsule, 1)\n",
    "            self.caps_layer = Caps_Layer()\n",
    "\n",
    "        def forward(self, x):\n",
    "\n",
    "            embeds = self.embedding(x)\n",
    "            embeds = torch.squeeze(self.embedding_dropout(torch.unsqueeze(embeds, 0)))\n",
    "\n",
    "            lstm_out, _ = self.lstm(embeds)\n",
    "            gru_out, _ = self.gru(lstm_out)\n",
    "\n",
    "            caps_out = self.caps_layer(gru_out)\n",
    "            caps_out = self.dropout(caps_out)\n",
    "            caps_out = caps_out.view(caps_out.size(0), -1)\n",
    "            caps_out = self.relu(self.lincaps(caps_out))\n",
    "\n",
    "            lstm_atten = self.lstm_atten(lstm_out)\n",
    "            gru_atten = self.gru_atten(gru_out)\n",
    "\n",
    "            avg_pool = torch.mean(gru_out, 1)\n",
    "            max_pool, _ = torch.max(gru_out, 1)\n",
    "            \n",
    "            conc = torch.cat((lstm_atten, gru_atten, caps_out, avg_pool, max_pool), 1)\n",
    "            conc = self.relu(self.linear(conc))\n",
    "            conc = self.dropout(conc)\n",
    "            out = self.out(conc)\n",
    "            return out\n",
    "\n",
    "\n",
    "    model = NeuralNet()\n",
    "    model.cuda()\n",
    "\n",
    "    class_weight = torch.FloatTensor([1.3]).cuda()\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss(reduction='sum', pos_weight=class_weight).cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.003)\n",
    "    scheduler = CyclicLR(optimizer, base_lr=0.001, max_lr=0.003, step_size=300, mode='exp_range', gamma=0.99994)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        test_preds = np.zeros((len(test_X)))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            y_pred = model(x_batch)\n",
    "            if scheduler:\n",
    "                scheduler.batch_step()\n",
    "            loss = loss_func(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if clip:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        test_preds_epoch = np.zeros(len(test_X))\n",
    "        avg_val_loss = 0.\n",
    "        if val:\n",
    "            for step, (x_batch, y_batch) in enumerate(test_loader):\n",
    "                y_pred = model(x_batch).detach()\n",
    "                loss = loss_func(y_pred, y_batch)\n",
    "                avg_val_loss += loss.item() / len(test_loader)\n",
    "                test_preds_epoch[step * batch_size:(step + 1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            test_preds += test_preds_epoch\n",
    "            \n",
    "            point = int(len(list(test_preds_epoch)) * 0.93065)\n",
    "            thresh = sorted(list(test_preds_epoch))[point]\n",
    "            pred_y = (test_preds_epoch > thresh).astype(int)\n",
    "            score = f1_score(y_pred=pred_y, y_true=test_y)\n",
    "            print('Epoch [{}/{}] - time: {:.2f}s - loss: {:.4f} - val_loss: {:.4f} - f1 score: {}'.format(epoch+1, epochs, time.time() - start_time, avg_loss, avg_val_loss, score))\n",
    "        else:\n",
    "            for step, (x_batch,) in enumerate(test_loader):\n",
    "                y_pred = model(x_batch).detach()\n",
    "                test_preds_epoch[step * batch_size:(step + 1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            test_preds += test_preds_epoch\n",
    "            print('Epoch [{}/{}] - time: {:.2f}s'.format(epoch+1, epochs, time.time() - start_time))\n",
    "        \n",
    "    return test_preds\n",
    "        \n",
    "\n",
    "# ****************************** model 2 ******************************\n",
    "def model2(epochs=4, embedding_matrix=None, clip=False):\n",
    "    \n",
    "    class NeuralNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(NeuralNet, self).__init__()\n",
    "            \n",
    "            hidden_size = 120\n",
    "            \n",
    "            self.embedding = nn.Embedding(max_features, embed_size)\n",
    "            self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "            self.embedding.weight.requires_grad = False\n",
    "            \n",
    "            self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "            self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "            self.gru = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "            \n",
    "            self.lstm_attention = Attention(hidden_size * 2, maxlen)\n",
    "            self.gru_attention = Attention(hidden_size * 2, maxlen)\n",
    "            \n",
    "            self.linear = nn.Linear(hidden_size * 8, 64)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.dropout = nn.Dropout(0.1)\n",
    "            self.out = nn.Linear(64, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            \n",
    "            embeds = self.embedding(x)\n",
    "            embeds = torch.squeeze(self.embedding_dropout(torch.unsqueeze(embeds, 0)))\n",
    "            \n",
    "            lstm_out, _ = self.lstm(embeds)\n",
    "            gru_out, _ = self.gru(lstm_out)\n",
    "            \n",
    "            lstm_atten = self.lstm_attention(lstm_out)\n",
    "            gru_atten = self.gru_attention(gru_out)\n",
    "            \n",
    "            avg_pool = torch.mean(gru_out, 1)\n",
    "            max_pool, _ = torch.max(gru_out, 1)\n",
    "            \n",
    "            conc = torch.cat((lstm_atten, gru_atten, avg_pool, max_pool), 1)\n",
    "            conc = self.relu(self.linear(conc))\n",
    "            conc = self.dropout(conc)\n",
    "            out = self.out(conc)\n",
    "            return out\n",
    "\n",
    "    model = NeuralNet()\n",
    "    model.cuda()\n",
    "    \n",
    "    class_weight = torch.FloatTensor([1.55]).cuda()\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss(reduction='sum', pos_weight=class_weight).cuda()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.003)\n",
    "    scheduler = CyclicLR(optimizer, base_lr=0.001, max_lr=0.003, step_size=300, mode='exp_range', gamma=0.99994)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        test_preds = np.zeros((len(test_X)))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            y_pred = model(x_batch)\n",
    "            if scheduler:\n",
    "                scheduler.batch_step()\n",
    "            loss = loss_func(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if clip:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        test_preds_epoch = np.zeros(len(test_X))\n",
    "        avg_val_loss = 0.\n",
    "        if val:\n",
    "            for step, (x_batch, y_batch) in enumerate(test_loader):\n",
    "                y_pred = model(x_batch).detach()\n",
    "                loss = loss_func(y_pred, y_batch)\n",
    "                avg_val_loss += loss.item() / len(test_loader)\n",
    "                test_preds_epoch[step * batch_size:(step + 1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            test_preds += test_preds_epoch\n",
    "            \n",
    "            point = int(len(list(test_preds_epoch)) * 0.93065)\n",
    "            thresh = sorted(list(test_preds_epoch))[point]\n",
    "            pred_y = (test_preds_epoch > thresh).astype(int)\n",
    "            score = f1_score(y_pred=pred_y, y_true=test_y)\n",
    "            print('Epoch [{}/{}] - time: {:.2f}s - loss: {:.4f} - val_loss: {:.4f} - f1 score: {}'.format(epoch+1, epochs, time.time() - start_time, avg_loss, avg_val_loss, score))\n",
    "        else:\n",
    "            for step, (x_batch,) in enumerate(test_loader):\n",
    "                y_pred = model(x_batch).detach()\n",
    "                test_preds_epoch[step * batch_size:(step + 1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            test_preds += test_preds_epoch\n",
    "            print('Epoch [{}/{}] - time: {:.2f}s'.format(epoch+1, epochs, time.time() - start_time))\n",
    "        \n",
    "    return test_preds\n",
    "\n",
    "\n",
    "print('>>> 1: ')\n",
    "pred0 = model1(4, embedding_matrix_gp)\n",
    "\n",
    "print('>>> 2: ')\n",
    "pred1 = model1(4, embedding_matrix_g)\n",
    "\n",
    "print('>>> 3: ')\n",
    "pred2 = model2(4, embedding_matrix_gp)\n",
    "\n",
    "print('>>> 4: ')\n",
    "pred3 = model2(4, embedding_matrix_g)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\">>>Total runtime %s.\" % (end_time - begin_time))\n",
    "# ****************************** submit ******************************\n",
    "if val:\n",
    "    # test_preds = pred0 + pred1 + pred2 + pred3\n",
    "    test_preds = 0.22598016 * pred0 + 0.17527643 * pred1 + 0.25782847 * pred2 + 0.28216464 * pred3\n",
    "    point = int(len(list(test_preds)) * 0.93065)\n",
    "    thresh = sorted(list(test_preds))[point]\n",
    "    pred_y = (test_preds > thresh).astype(int)\n",
    "    score = f1_score(y_pred=pred_y, y_true=test_y)\n",
    "    print(\"  f1 score :\", score)\n",
    "\n",
    "else:\n",
    "    df_test = pd.read_csv(\"../input/test.csv\")\n",
    "    submission = df_test[['qid']].copy()\n",
    "    # test_preds = pred0 + pred1 + pred2 + pred3\n",
    "    test_preds = 0.22598016 * pred0 + 0.17527643 * pred1 + 0.25782847 * pred2 + 0.28216464 * pred3\n",
    "    point = int(len(list(test_preds)) * 0.9305246930441406)\n",
    "    thresh = sorted(list(test_preds))[point]\n",
    "    print(\" 阀值:\",thresh)\n",
    "    submission['prediction'] = (test_preds > thresh).astype(int)\n",
    "    submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
